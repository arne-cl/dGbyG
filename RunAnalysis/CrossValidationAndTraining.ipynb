{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dGbyG.utils.custom_tools import rapid_process_result, rapid_linear_reg\n",
    "from dGbyG.network.Dataset import Train_Dataset\n",
    "from dGbyG.network.GNNetwork import MP_network\n",
    "from dGbyG.train.trainer import Model\n",
    "from dGbyG.config import train_data_path, inference_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the train data and network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingData_df = pd.read_csv(train_data_path)\n",
    "mean_std = TrainingData_df.loc[:,'std'].mean()\n",
    "\n",
    "Scale = []\n",
    "for n, sem in zip(TrainingData_df.loc[:,'n'], TrainingData_df.loc[:,'SEM']):\n",
    "    if np.isnan(sem):\n",
    "        scale = mean_std\n",
    "    else:\n",
    "        scale = (sem**2 + mean_std**2/n)**0.5\n",
    "    Scale.append(scale)\n",
    "Scale = np.array(Scale)\n",
    "\n",
    "#SEM = np.nan_to_num(TrainingData_df.loc[:,'SEM'], nan=mean_std)\n",
    "\n",
    "equation = TrainingData_df.loc[:, 'reaction']\n",
    "standard_dG_prime = TrainingData_df.loc[:, 'standard_dg_prime']\n",
    "weight = 1/np.array(Scale)/np.median(Scale) #(1/(SEM+1))/np.median((1/(SEM+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = '../data/results_data/cross_validation_results/10_fold_cross_validation/'\n",
    "for n in range(100):\n",
    "    name = os.path.join(results_dir, str(n))\n",
    "    if not os.path.exists(name+'.csv'):\n",
    "        print(n)\n",
    "        dG = standard_dG_prime + np.random.randn(standard_dG_prime.shape[0]) * Scale\n",
    "        TrainSet = Train_Dataset(equations=equation, dGs=dG, weights=weight)\n",
    "\n",
    "        network = MP_network(atom_dim=TrainSet[0].x.size(1), bond_dim=TrainSet[0].edge_attr.size(1), emb_dim=300, num_layer=2)\n",
    "        model = Model()\n",
    "        model.network = network\n",
    "        Loss, Result_df = model.cross_validation(TrainSet, mode=10, epochs=9000, lr=1e-4, weight_decay=1e-6)\n",
    "        Result_df = pd.concat([pd.Series(standard_dG_prime), Result_df], axis=1)\n",
    "\n",
    "        np.save(name+'.npy', Loss), Result_df.to_csv(name+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10-fold cross validation of unweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = '../data/results_data/cross_validation_results/10_fold_cross_validation_unweighing/'\n",
    "for n in range(100):\n",
    "    name = os.path.join(results_dir, str(n))\n",
    "    if not os.path.exists(name+'.csv'):\n",
    "        print(n)\n",
    "        dG = standard_dG_prime + np.random.randn(standard_dG_prime.shape[0]) * Scale\n",
    "        TrainSet = Train_Dataset(equations=equation, dGs=dG, weights=None)\n",
    "\n",
    "        network = MP_network(atom_dim=TrainSet[0].x.size(1), bond_dim=TrainSet[0].edge_attr.size(1), emb_dim=300, num_layer=2)\n",
    "        model = Model()\n",
    "        model.network = network\n",
    "        Loss, Result_df = model.cross_validation(TrainSet, mode=10, epochs=9000, lr=1e-4, weight_decay=1e-6)\n",
    "        Result_df = pd.concat([pd.Series(standard_dG_prime), Result_df], axis=1)\n",
    "\n",
    "        np.save(name+'.npy', Loss), Result_df.to_csv(name+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10-fold cross validation without random error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = '../data/results_data/cross_validation_results/10_fold_cross_validation_without_random_dG/'\n",
    "for n in range(100):\n",
    "    name = os.path.join(results_dir, str(n))\n",
    "    if not os.path.exists(name+'.csv'):\n",
    "        print(n)\n",
    "        TrainSet = Train_Dataset(equations=equation, dGs=standard_dG_prime, weights=weight)\n",
    "\n",
    "        network = MP_network(atom_dim=TrainSet[0].x.size(1), bond_dim=TrainSet[0].edge_attr.size(1), emb_dim=300, num_layer=2)\n",
    "        model = Model()\n",
    "        model.network = network\n",
    "        Loss, Result_df = model.cross_validation(TrainSet, mode=10, epochs=9000, lr=1e-4, weight_decay=1e-6)\n",
    "        Result_df = pd.concat([pd.Series(standard_dG_prime), Result_df], axis=1)\n",
    "\n",
    "        np.save(name+'.npy', Loss), Result_df.to_csv(name+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10-fold cross validation of unweighing and without random error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = '../data/results_data/cross_validation_results/10_fold_cross_validation_unweighing_without_random_dG'\n",
    "for n in range(100):\n",
    "    name = os.path.join(results_dir, str(n))\n",
    "    if not os.path.exists(name+'.csv'):\n",
    "        print(n)\n",
    "        TrainSet = Train_Dataset(equations=equation, dGs=standard_dG_prime, weights=None)\n",
    "\n",
    "        network = MP_network(atom_dim=TrainSet[0].x.size(1), bond_dim=TrainSet[0].edge_attr.size(1), emb_dim=300, num_layer=2)\n",
    "        model = Model()\n",
    "        model.network = network\n",
    "        Loss, Result_df = model.cross_validation(TrainSet, mode=10, epochs=9000, lr=1e-4, weight_decay=1e-6)\n",
    "        Result_df = pd.concat([pd.Series(standard_dG_prime), Result_df], axis=1)\n",
    "\n",
    "        np.save(name+'.npy', Loss), Result_df.to_csv(name+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = '../data/results_data/cross_validation_results/5_fold_cross_validation/'\n",
    "for n in range(20):\n",
    "    name = os.path.join(results_dir, str(n))\n",
    "    if not os.path.exists(name+'.csv'):\n",
    "        print(n)\n",
    "        dG = standard_dG_prime + np.random.randn(standard_dG_prime.shape[0]) * Scale\n",
    "        TrainSet = Train_Dataset(equations=equation, dGs=dG, weights=weight)\n",
    "\n",
    "        network = MP_network(atom_dim=TrainSet[0].x.size(1), bond_dim=TrainSet[0].edge_attr.size(1), emb_dim=300, num_layer=2)\n",
    "        model = Model()\n",
    "        model.network = network\n",
    "        Loss, Result_df = model.cross_validation(TrainSet, mode=5, epochs=9000, lr=1e-4, weight_decay=1e-6)\n",
    "        Result_df = pd.concat([pd.Series(standard_dG_prime), Result_df], axis=1)\n",
    "\n",
    "        np.save(name+'.npy', Loss), Result_df.to_csv(name+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = '../data/results_data/cross_validation_results/2_fold_cross_validation/'\n",
    "for n in range(20):\n",
    "    name = os.path.join(results_dir, str(n))\n",
    "    if not os.path.exists(name+'.csv'):\n",
    "        print(n)\n",
    "        dG = standard_dG_prime + np.random.randn(standard_dG_prime.shape[0]) * Scale\n",
    "        TrainSet = Train_Dataset(equations=equation, dGs=dG, weights=weight)\n",
    "\n",
    "        network = MP_network(atom_dim=TrainSet[0].x.size(1), bond_dim=TrainSet[0].edge_attr.size(1), emb_dim=300, num_layer=2)\n",
    "        model = Model()\n",
    "        model.network = network\n",
    "        Loss, Result_df = model.cross_validation(TrainSet, mode=2, epochs=9000, lr=1e-4, weight_decay=1e-6)\n",
    "        Result_df = pd.concat([pd.Series(standard_dG_prime), Result_df], axis=1)\n",
    "\n",
    "        np.save(name+'.npy', Loss), Result_df.to_csv(name+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(100):\n",
    "    name = '10_fold_cross_validation_with_random_dG_'+str(n)\n",
    "    dG = standard_dG_prime + np.random.randn(standard_dG_prime.shape[0]) * Scale\n",
    "    TrainSet = Train_Dataset(equations=equation, dGs=dG, weights=weight)\n",
    "\n",
    "    network = MP_network(atom_dim=TrainSet[0].x.size(1), bond_dim=TrainSet[0].edge_attr.size(1), emb_dim=300, num_layer=2)\n",
    "    model = Model()\n",
    "    model.network = network\n",
    "\n",
    "    loss_history, Result_df, i = model.train(TrainSet, 9000, 1e-4, 1e-6)\n",
    "    torch.save(model.network.state_dict(), '../network/best_model_params/'+str(n)+'.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dGbyG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
