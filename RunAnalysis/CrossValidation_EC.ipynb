{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dGbyG.utils.custom_tools import rapid_process_result, rapid_linear_reg\n",
    "from dGbyG.network.Dataset import Train_Dataset\n",
    "from dGbyG.network.GNNetwork import MP_network\n",
    "from dGbyG.train.trainer import Model\n",
    "from dGbyG.config import train_data_path, inference_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the train data and network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingData_df = pd.read_csv(train_data_path)\n",
    "mean_std = TrainingData_df.loc[:,'std'].mean()\n",
    "\n",
    "Scale = []\n",
    "for n, sem in zip(TrainingData_df.loc[:,'n'], TrainingData_df.loc[:,'SEM']):\n",
    "    if np.isnan(sem):\n",
    "        scale = mean_std\n",
    "    else:\n",
    "        scale = (sem**2 + mean_std**2/n)**0.5\n",
    "    Scale.append(scale)\n",
    "Scale = np.array(Scale)\n",
    "\n",
    "equation = TrainingData_df.loc[:, 'reaction']\n",
    "standard_dG_prime = TrainingData_df.loc[:, 'standard_dg_prime']\n",
    "weight = 1/np.array(Scale)/np.median(Scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_by_ec(ec_startswith):\n",
    "    children, not_children = [], []\n",
    "    all_class = set()\n",
    "    for EC in TrainingData_df.loc[:, 'EC']:\n",
    "        if pd.isna(EC):\n",
    "            children.append(False), not_children.append(False)\n",
    "            continue\n",
    "\n",
    "        EC = eval(EC)\n",
    "        remove_ec, add_ec = set(), set()\n",
    "        for ec in EC:\n",
    "            if '&' in ec:\n",
    "                remove_ec.add(ec)\n",
    "                add_ec |= set(ec.split('&'))\n",
    "        EC = (set(EC) | add_ec) - set(remove_ec)\n",
    "\n",
    "        child, not_child = True, True\n",
    "        for ec in EC:\n",
    "            if not ec.startswith(ec_startswith):\n",
    "                child = False\n",
    "            else:\n",
    "                not_child = False\n",
    "        children.append(child), not_children.append(not_child)\n",
    "        all_class |= EC\n",
    "    return children, not_children, all_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EC cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[116, 333, 449],\n",
       " [146, 302, 448],\n",
       " [49, 401, 450],\n",
       " [58, 392, 450],\n",
       " [65, 385, 450],\n",
       " [10, 443, 453]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ec_classify_num = []\n",
    "for n in range(1,7):\n",
    "    children, not_children, all_class = classify_by_ec(str(n))\n",
    "    ec_classify_num.append([sum(children), sum(not_children), sum(children)+sum(not_children)])\n",
    "ec_classify_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "formation_idx = np.where(TrainingData_df.loc[:, 'type'] == 'formation dg')[0]\n",
    "Mode = 'manual'\n",
    "for ec in range(1,7):\n",
    "    children, not_children, all_class = classify_by_ec(str(ec))\n",
    "    train_idx = list(formation_idx) + list(np.where(not_children)[0])\n",
    "    val_idx = np.where(children)[0]\n",
    "    \n",
    "    results_dir = '../data/results_data/cross_validation_results/EC{0}_cross_validation/'.format(ec)\n",
    "    if not os.path.isdir(results_dir):\n",
    "        os.mkdir(results_dir)\n",
    "    for n in range(20):\n",
    "        name = os.path.join(results_dir, str(n))\n",
    "        if not os.path.exists(name+'.csv'):\n",
    "            print(n)\n",
    "            dG = standard_dG_prime + np.random.randn(standard_dG_prime.shape[0]) * Scale\n",
    "            TrainSet = Train_Dataset(equations=equation, dGs=dG, weights=weight)\n",
    "\n",
    "            model = Model()\n",
    "            model.network = MP_network(atom_dim=TrainSet[0].x.size(1), bond_dim=TrainSet[0].edge_attr.size(1), emb_dim=300, num_layer=2)\n",
    "            Loss, Result_df = model.cross_validation(TrainSet, mode=Mode, train_idx=train_idx, val_idx=val_idx, epochs=9000, lr=1e-4, weight_decay=1e-6)\n",
    "            Result_df = pd.concat([pd.Series(standard_dG_prime), Result_df], axis=1)\n",
    "\n",
    "            np.save(name+'.npy', Loss), Result_df.to_csv(name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_dir = '../data/results_data/cross_validation_results/EC{0}_cross_validation/'.format(ec)\n",
    "if not os.path.isdir(results_dir):\n",
    "\tos.mkdir(results_dir)\n",
    "for n in range(20):\n",
    "    name = os.path.join(results_dir, str(n))\n",
    "    if not os.path.exists(name+'.csv'):\n",
    "        print(n)\n",
    "        dG = standard_dG_prime + np.random.randn(standard_dG_prime.shape[0]) * Scale\n",
    "        TrainSet = Train_Dataset(equations=equation, dGs=dG, weights=weight)\n",
    "\n",
    "        network = MP_network(atom_dim=TrainSet[0].x.size(1), bond_dim=TrainSet[0].edge_attr.size(1), emb_dim=300, num_layer=2)\n",
    "        model = Model()\n",
    "        model.network = network\n",
    "        Loss, Result_df = model.cross_validation(TrainSet, mode=Mode, train_idx=formation_idx, val_idx=reaction_idx, epochs=9000, lr=1e-4, weight_decay=1e-6)\n",
    "        Result_df = pd.concat([pd.Series(standard_dG_prime), Result_df], axis=1)\n",
    "\n",
    "        np.save(name+'.npy', Loss), Result_df.to_csv(name+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formation-reaction cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "formation_idx = np.where(TrainingData_df.loc[:, 'type'] == 'formation dg')[0]\n",
    "reaction_idx = np.where(TrainingData_df.loc[:, 'type'] == 'reaction dg')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mode = 'manual'\n",
    "results_dir = '../data/results_data/cross_validation_results/formation2reaction_cross_validation/'\n",
    "if not os.path.isdir(results_dir):\n",
    "\tos.mkdir(results_dir)\n",
    "for n in range(20):\n",
    "    name = os.path.join(results_dir, str(n))\n",
    "    if not os.path.exists(name+'.csv'):\n",
    "        print(n)\n",
    "        dG = standard_dG_prime + np.random.randn(standard_dG_prime.shape[0]) * Scale\n",
    "        TrainSet = Train_Dataset(equations=equation, dGs=dG, weights=weight)\n",
    "\n",
    "        network = MP_network(atom_dim=TrainSet[0].x.size(1), bond_dim=TrainSet[0].edge_attr.size(1), emb_dim=300, num_layer=2)\n",
    "        model = Model()\n",
    "        model.network = network\n",
    "        Loss, Result_df = model.cross_validation(TrainSet, mode=Mode, train_idx=formation_idx, val_idx=reaction_idx, epochs=9000, lr=1e-4, weight_decay=1e-6)\n",
    "        Result_df = pd.concat([pd.Series(standard_dG_prime), Result_df], axis=1)\n",
    "\n",
    "        np.save(name+'.npy', Loss), Result_df.to_csv(name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mode = 'manual'\n",
    "results_dir = '../data/results_data/cross_validation_results/reaction2formation_cross_validation/'\n",
    "if not os.path.isdir(results_dir):\n",
    "\tos.mkdir(results_dir)\n",
    "for n in range(20):\n",
    "    name = os.path.join(results_dir, str(n))\n",
    "    if not os.path.exists(name+'.csv'):\n",
    "        print(n)\n",
    "        dG = standard_dG_prime + np.random.randn(standard_dG_prime.shape[0]) * Scale\n",
    "        TrainSet = Train_Dataset(equations=equation, dGs=dG, weights=weight)\n",
    "\n",
    "        network = MP_network(atom_dim=TrainSet[0].x.size(1), bond_dim=TrainSet[0].edge_attr.size(1), emb_dim=300, num_layer=2)\n",
    "        model = Model()\n",
    "        model.network = network\n",
    "        Loss, Result_df = model.cross_validation(TrainSet, mode=Mode, train_idx=reaction_idx, val_idx=formation_idx, epochs=9000, lr=1e-4, weight_decay=1e-6)\n",
    "        Result_df = pd.concat([pd.Series(standard_dG_prime), Result_df], axis=1)\n",
    "\n",
    "        np.save(name+'.npy', Loss), Result_df.to_csv(name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dGbyG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
